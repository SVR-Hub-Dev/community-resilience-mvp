# Revised Deployment Plan: Hybrid Cloud + Local Resilience

## Architecture Overview

| Component | Cloud Deployment | Local Offline Instance | Sync Strategy |
| --------- | ---------------- | ---------------------- | ------------- |
| Frontend | Vercel | Local SvelteKit build | Service Worker + IndexedDB |
| Backend | Render | Local FastAPI (Docker) | REST API sync |
| Database | Neon | Local PostgreSQL (Docker) | Bidirectional sync |
| LLM | Groq API | Ollama (local) | Fallback routing |
| Document Processing | Basic (PyMuPDF) | Full (Docling) | Process locally, sync results |

## Document Processing Strategy

### Why Hybrid Processing?

Render's free tier has 512MB RAM, which is insufficient for full Docling processing that requires:

- Tesseract OCR (~200MB)
- LibreOffice (~500MB+)
- Docling itself (~100MB)

**Solution**: Cloud accepts uploads with basic text extraction; local instances perform full processing and sync results back.

### Processing Flow

```text
┌─────────────────────────────────────────────────────────────────────┐
│                     DOCUMENT PROCESSING FLOW                        │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  1. UPLOAD (Cloud)                                                  │
│     User uploads document ──▶ Basic PyMuPDF extraction              │
│                              ──▶ Store with needs_full_processing=true│
│                                                                      │
│  2. SYNC (Local pulls from Cloud)                                   │
│     Local instance ──▶ GET /api/sync/documents/unprocessed          │
│                    ──▶ Downloads raw files                          │
│                                                                      │
│  3. PROCESS (Local)                                                 │
│     Docling ──▶ Full text extraction                                │
│            ──▶ OCR for scanned documents                            │
│            ──▶ Office document conversion                           │
│            ──▶ Section structure extraction                         │
│                                                                      │
│  4. SYNC (Local pushes to Cloud)                                    │
│     Local instance ──▶ POST /api/sync/documents/{id}/processed      │
│                    ──▶ Cloud updates document                       │
│                    ──▶ Sets needs_full_processing=false             │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

## Key Changes Needed

### 1. **Database Schema Modifications for Sync**

Add these tables to the existing schema:

```sql
-- Add to migrations
CREATE TABLE sync_metadata (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    entity_type VARCHAR(50) NOT NULL,  -- 'document', 'user', 'update'
    entity_id UUID NOT NULL,
    last_modified TIMESTAMPTZ DEFAULT NOW(),
    sync_version INTEGER DEFAULT 1,
    instance_id UUID NOT NULL,
    is_deleted BOOLEAN DEFAULT FALSE,
    cloud_synced BOOLEAN DEFAULT FALSE,
    local_synced BOOLEAN DEFAULT FALSE
);

CREATE TABLE sync_log (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    instance_id UUID NOT NULL,
    sync_timestamp TIMESTAMPTZ DEFAULT NOW(),
    direction VARCHAR(10) CHECK (direction IN ('push', 'pull')),
    entities_count INTEGER DEFAULT 0,
    success BOOLEAN DEFAULT TRUE
);

-- Add document processing fields
ALTER TABLE documents ADD COLUMN IF NOT EXISTS needs_full_processing BOOLEAN DEFAULT FALSE;
ALTER TABLE documents ADD COLUMN IF NOT EXISTS processing_mode VARCHAR(20) DEFAULT 'cloud_simple';
ALTER TABLE documents ADD COLUMN IF NOT EXISTS raw_file_path TEXT;
ALTER TABLE documents ADD COLUMN IF NOT EXISTS processed_at TIMESTAMPTZ;
```

### 2. **Backend Sync Endpoints**

Add to the FastAPI backend:

```python
# backend/app/api/sync.py
from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel
from typing import List, Optional
from datetime import datetime
from uuid import UUID

router = APIRouter(prefix="/api/sync", tags=["sync"])

class SyncChange(BaseModel):
    entity_type: str
    entity_id: UUID
    operation: str  # 'create', 'update', 'delete'
    data: Optional[dict]
    last_modified: datetime

class SyncPayload(BaseModel):
    instance_id: UUID
    changes: List[SyncChange]
    last_sync: Optional[datetime]

class ProcessedDocument(BaseModel):
    content: str
    metadata: dict
    sections: List[dict]

@router.post("/push")
async def push_changes(payload: SyncPayload, api_key: str = Depends(verify_sync_key)):
    """Accept changes from local instance."""
    results = []
    for change in payload.changes:
        result = await apply_change(change)
        results.append(result)
    
    await log_sync(payload.instance_id, 'push', len(payload.changes))
    return {"applied": len(results), "conflicts": []}
    
@router.get("/pull")
async def pull_changes(
    last_sync: Optional[datetime] = None,
    api_key: str = Depends(verify_sync_key)
):
    """Return changes since last_sync."""
    changes = await get_changes_since(last_sync)
    return {"changes": changes, "timestamp": datetime.utcnow()}

@router.get("/documents/unprocessed")
async def get_unprocessed_documents(api_key: str = Depends(verify_sync_key)):
    """Get documents that need full Docling processing."""
    documents = await db.fetch_all("""
        SELECT id, filename, raw_file_path, category, tags, created_at
        FROM documents 
        WHERE needs_full_processing = TRUE
        ORDER BY created_at ASC
        LIMIT 50
    """)
    return {"documents": documents}

@router.post("/documents/{document_id}/processed")
async def receive_processed_document(
    document_id: UUID,
    data: ProcessedDocument,
    api_key: str = Depends(verify_sync_key)
):
    """Receive fully processed document from local instance."""
    await db.execute("""
        UPDATE documents 
        SET content = :content,
            metadata = :metadata,
            sections = :sections,
            needs_full_processing = FALSE,
            processing_mode = 'local_full',
            processed_at = NOW()
        WHERE id = :document_id
    """, {
        'document_id': document_id,
        'content': data.content,
        'metadata': data.metadata,
        'sections': data.sections
    })
    
    # Re-generate embeddings with full content
    await regenerate_embeddings(document_id, data.content)
    
    return {"status": "updated", "document_id": document_id}
```

### 3. **Dual LLM Provider Strategy**

Modify the LLM service to support fallback:

```python
# backend/services/llm_service.py
from config import DeploymentConfig, DeploymentMode

class DualLLMService:
    def __init__(self):
        if DeploymentConfig.DEPLOYMENT_MODE == DeploymentMode.CLOUD:
            from services.groq_client import GroqClient
            self.primary_client = GroqClient()
            self.fallback_client = None
        else:
            from services.ollama_client import OllamaClient
            self.primary_client = OllamaClient()
            self.fallback_client = None
            
            # Optional: also configure Groq as fallback for local
            if os.getenv("GROQ_API_KEY"):
                from services.groq_client import GroqClient
                self.fallback_client = GroqClient()
        
    async def generate_response(self, prompt: str) -> str:
        try:
            return await self.primary_client.generate(prompt)
        except Exception as e:
            if self.fallback_client:
                return await self.fallback_client.generate(prompt)
            raise e
```

### 4. **Local Deployment Configuration**

Create `docker-compose.local.yml`:

```yaml
version: '3.8'
services:
  database:
    image: ankane/pgvector:latest
    environment:
      POSTGRES_DB: community_resilience
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./seed_data:/docker-entrypoint-initdb.d

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        limits:
          memory: 4G

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.local
    environment:
      DATABASE_URL: postgresql://postgres:postgres@database:5432/community_resilience
      OLLAMA_URL: http://ollama:11434
      LLM_PROVIDER: ollama
      DEPLOYMENT_MODE: local
      SYNC_ENABLED: ${SYNC_ENABLED:-false}
      SYNC_SERVER_URL: ${SYNC_SERVER_URL:-}
      SYNC_API_KEY: ${SYNC_API_KEY:-}
    ports:
      - "8000:8000"
    volumes:
      - document_storage:/app/documents
    depends_on:
      - database
      - ollama
    deploy:
      resources:
        limits:
          memory: 4G

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.local
    environment:
      VITE_API_URL: http://backend:8000
    ports:
      - "3000:3000"
    depends_on:
      - backend

  # Background sync worker (optional)
  sync-worker:
    build:
      context: ./backend
      dockerfile: Dockerfile.local
    command: python -m services.sync_worker
    environment:
      DATABASE_URL: postgresql://postgres:postgres@database:5432/community_resilience
      DEPLOYMENT_MODE: local
      SYNC_ENABLED: ${SYNC_ENABLED:-false}
      SYNC_SERVER_URL: ${SYNC_SERVER_URL:-}
      SYNC_API_KEY: ${SYNC_API_KEY:-}
      SYNC_INTERVAL_MINUTES: ${SYNC_INTERVAL_MINUTES:-15}
    volumes:
      - document_storage:/app/documents
    depends_on:
      - database
      - backend
    profiles:
      - sync  # Only starts with: docker-compose --profile sync up

volumes:
  postgres_data:
  ollama_data:
  document_storage:
```

### 5. **Sync Worker for Background Processing**

```python
# backend/services/sync_worker.py
import asyncio
import os
import logging
from datetime import datetime
from pathlib import Path

from services.document_sync import DocumentSyncService
from services.docling_service import DoclingProcessor

logger = logging.getLogger(__name__)

class SyncWorker:
    def __init__(self):
        self.sync_service = DocumentSyncService()
        self.docling = DoclingProcessor()
        self.sync_interval = int(os.getenv("SYNC_INTERVAL_MINUTES", 15)) * 60
        self.cloud_url = os.getenv("SYNC_SERVER_URL")
        self.api_key = os.getenv("SYNC_API_KEY")
    
    async def run(self):
        """Main sync loop."""
        logger.info(f"Sync worker started. Interval: {self.sync_interval}s")
        
        while True:
            try:
                await self.sync_cycle()
            except Exception as e:
                logger.error(f"Sync cycle failed: {e}")
            
            await asyncio.sleep(self.sync_interval)
    
    async def sync_cycle(self):
        """Single sync cycle: pull, process, push."""
        logger.info("Starting sync cycle...")
        
        # 1. Pull unprocessed documents from cloud
        unprocessed = await self.sync_service.pull_unprocessed_documents(
            self.cloud_url, self.api_key
        )
        logger.info(f"Pulled {len(unprocessed)} unprocessed documents")
        
        # 2. Process each document with Docling
        processed = []
        for doc in unprocessed:
            try:
                result = await self.process_document(doc)
                if result['success']:
                    processed.append({
                        'id': doc['id'],
                        **result
                    })
            except Exception as e:
                logger.error(f"Failed to process {doc['id']}: {e}")
        
        # 3. Push processed documents back to cloud
        if processed:
            await self.sync_service.push_processed_documents(
                self.cloud_url, self.api_key, processed
            )
            logger.info(f"Pushed {len(processed)} processed documents")
        
        logger.info("Sync cycle complete")
    
    async def process_document(self, doc: dict) -> dict:
        """Process a single document with Docling."""
        # Download raw file if needed
        file_path = Path(doc.get('raw_file_path', ''))
        if not file_path.exists():
            file_path = await self.download_file(doc)
        
        # Process with Docling
        result = await self.docling.process_document(file_path)
        return result

if __name__ == "__main__":
    worker = SyncWorker()
    asyncio.run(worker.run())
```

### 6. **Frontend Offline-First Strategy**

Add service worker and sync logic:

```javascript
// frontend/src/lib/offline.js
export class OfflineManager {
    constructor() {
        this.pendingChanges = [];
        this.isOnline = navigator.onLine;
        this.setupEventListeners();
    }
    
    setupEventListeners() {
        window.addEventListener('online', () => {
            this.isOnline = true;
            this.syncChanges();
        });
        
        window.addEventListener('offline', () => {
            this.isOnline = false;
        });
    }
    
    async queueChange(change) {
        this.pendingChanges.push({
            ...change,
            timestamp: new Date().toISOString()
        });
        await this.saveToIndexedDB('pending_changes', this.pendingChanges);
        
        if (this.isOnline) {
            await this.syncChanges();
        }
    }
    
    async syncChanges() {
        const changes = await this.getPendingChanges();
        if (changes.length === 0) return;
        
        try {
            const response = await fetch('/api/sync/push', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ 
                    changes,
                    instance_id: this.getInstanceId()
                })
            });
            
            if (response.ok) {
                await this.clearPendingChanges();
            }
        } catch (error) {
            console.error('Sync failed:', error);
        }
    }
    
    getInstanceId() {
        let id = localStorage.getItem('instance_id');
        if (!id) {
            id = crypto.randomUUID();
            localStorage.setItem('instance_id', id);
        }
        return id;
    }
}
```

## Revised Deployment Steps

### Phase 1: Enhanced Cloud Deployment

1. **Deploy to cloud as planned** with sync endpoints
2. **Set DEPLOYMENT_MODE=cloud** to use lightweight processing
3. **Generate SYNC_API_KEY** for local instances
4. **Test cloud functionality** (basic document upload works)

### Phase 2: Local Instance Deployment

1. **Create local Docker setup** with the compose file above
2. **Build local images** with full Docling dependencies
3. **Deploy with sync enabled**:

```bash
#!/bin/bash
# deploy-local.sh
echo "Setting up local resilience hub instance..."

# Configure sync
export SYNC_ENABLED=true
export SYNC_SERVER_URL="https://your-cloud-backend.onrender.com"
export SYNC_API_KEY="your-secure-api-key"

# Start local services (with sync worker)
docker-compose -f docker-compose.local.yml --profile sync up -d

# Wait for services to be ready
echo "Waiting for services to initialize..."
sleep 60

# Run migrations
docker exec community-resilience-backend alembic upgrade head

# Pull initial data from cloud
docker exec community-resilience-backend python scripts/initial_sync.py

echo "Local instance ready at http://localhost:3000"
```

### Phase 3: Sync Strategy Implementation

1. **Background sync service** runs every 15 minutes when online
2. **Document processing queue** handles pending files
3. **Conflict resolution** for manual review

## Cost Analysis - Still Free Tier Friendly

| Service | Cloud | Local | Cost |
| ------- | ----- | ----- | ---- |
| **Database** | Neon (free) | PostgreSQL Docker (free) | $0 |
| **LLM** | Groq (free) | Ollama (free) | $0 |
| **Backend** | Render (free) | Local Docker (free) | $0 |
| **Frontend** | Vercel (free) | Local build (free) | $0 |
| **Document Processing** | Basic only | Full Docling | $0 |

## Emergency Operation Protocol

### Normal Operation (Online)

- Local instance syncs with cloud every 15 minutes
- Uses Groq API for faster responses (or Ollama locally)
- Documents uploaded to cloud get basic processing
- Local instance processes queue and syncs results

### Emergency Mode (Offline)

- Local instance operates independently
- Uses Ollama for LLM capabilities
- Full Docling processing available
- Queues changes for later sync

### Recovery (Back Online)

- Automatic sync resumes
- Pending documents processed and synced
- Conflict detection for overlapping changes

## Implementation Priority

1. **Week 1**: Add sync endpoints and DEPLOYMENT_MODE config
2. **Week 2**: Create local Docker setup with Docling
3. **Week 3**: Implement sync worker and document processing queue
4. **Week 4**: Frontend offline capabilities and testing
